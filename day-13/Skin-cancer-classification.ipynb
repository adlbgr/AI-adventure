{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Cancer\", \"Non_Cancer\"]\n",
    "img_path = \"skin-cancer/\"\n",
    "\n",
    "img_list = []\n",
    "label_list = []\n",
    "\n",
    "for label in labels:\n",
    "    for img_file in os.listdir(img_path + label):\n",
    "        img_list.append(img_path + label + \"/\" + img_file)\n",
    "        label_list.append(label)\n",
    "\n",
    "df = pd.DataFrame({'img' : img_list, 'label' : label_list})\n",
    "# df.to_csv(\"skin-cancer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skin-cancer/Cancer/1007-1.jpg</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skin-cancer/Cancer/1010-01.JPG</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skin-cancer/Cancer/1012-2.JPG</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>skin-cancer/Cancer/1031-1.jpg</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skin-cancer/Cancer/1051-3(94).jpg</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 img   label\n",
       "0      skin-cancer/Cancer/1007-1.jpg  Cancer\n",
       "1     skin-cancer/Cancer/1010-01.JPG  Cancer\n",
       "2      skin-cancer/Cancer/1012-2.JPG  Cancer\n",
       "3      skin-cancer/Cancer/1031-1.jpg  Cancer\n",
       "4  skin-cancer/Cancer/1051-3(94).jpg  Cancer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Cancer' : 1, 'Non_Cancer' : 0}\n",
    "df['cancer'] = df['label'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>cancer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>skin-cancer/Cancer/1545.JPG</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>skin-cancer/Non_Cancer/1589-01.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>skin-cancer/Non_Cancer/2566-03.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>skin-cancer/Non_Cancer/638-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>skin-cancer/Cancer/2632-4.JPG</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    img       label  cancer\n",
       "16          skin-cancer/Cancer/1545.JPG      Cancer       1\n",
       "137  skin-cancer/Non_Cancer/1589-01.JPG  Non_Cancer       0\n",
       "201  skin-cancer/Non_Cancer/2566-03.JPG  Non_Cancer       0\n",
       "245    skin-cancer/Non_Cancer/638-1.JPG  Non_Cancer       0\n",
       "65        skin-cancer/Cancer/2632-4.JPG      Cancer       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "for img in df['img']:\n",
    "    img = cv2.imread(img) # Read image\n",
    "    img = cv2.resize(img, (170, 170)) # Resize image\n",
    "    img =img/ 255.0 # Normalize image\n",
    "    X.append(img) # Append image\n",
    "\n",
    "X = np.array(X) # Convert list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['cancer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "283    0\n",
       "284    0\n",
       "285    0\n",
       "286    0\n",
       "287    0\n",
       "Name: cancer, Length: 288, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization,InputLayer\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(170, 170, 3))) # we said shape, so we don't have to specify it in the next layers.(Reshape)\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax')) # 2 because we have 2 classes\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 594ms/step - accuracy: 0.4833 - loss: 3.4403 - val_accuracy: 0.7414 - val_loss: 0.7502\n",
      "Epoch 2/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 469ms/step - accuracy: 0.7255 - loss: 0.6300 - val_accuracy: 0.7586 - val_loss: 0.6490\n",
      "Epoch 3/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 451ms/step - accuracy: 0.8147 - loss: 0.5887 - val_accuracy: 0.3448 - val_loss: 1.2143\n",
      "Epoch 4/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 428ms/step - accuracy: 0.6293 - loss: 0.6706 - val_accuracy: 0.7414 - val_loss: 0.5375\n",
      "Epoch 5/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 421ms/step - accuracy: 0.7448 - loss: 0.5202 - val_accuracy: 0.7586 - val_loss: 0.5196\n",
      "Epoch 6/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 443ms/step - accuracy: 0.7824 - loss: 0.4565 - val_accuracy: 0.7759 - val_loss: 0.4920\n",
      "Epoch 7/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 429ms/step - accuracy: 0.8095 - loss: 0.4088 - val_accuracy: 0.7759 - val_loss: 0.5566\n",
      "Epoch 8/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 428ms/step - accuracy: 0.8631 - loss: 0.3656 - val_accuracy: 0.7759 - val_loss: 0.4108\n",
      "Epoch 9/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 421ms/step - accuracy: 0.8500 - loss: 0.3723 - val_accuracy: 0.8448 - val_loss: 0.4444\n",
      "Epoch 10/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 420ms/step - accuracy: 0.8719 - loss: 0.3413 - val_accuracy: 0.8448 - val_loss: 0.4086\n",
      "Epoch 11/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 429ms/step - accuracy: 0.8925 - loss: 0.2460 - val_accuracy: 0.8621 - val_loss: 0.3381\n",
      "Epoch 12/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 443ms/step - accuracy: 0.8731 - loss: 0.2692 - val_accuracy: 0.8103 - val_loss: 0.4923\n",
      "Epoch 13/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 425ms/step - accuracy: 0.8748 - loss: 0.2773 - val_accuracy: 0.8793 - val_loss: 0.3347\n",
      "Epoch 14/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 429ms/step - accuracy: 0.9303 - loss: 0.2290 - val_accuracy: 0.9138 - val_loss: 0.2799\n",
      "Epoch 15/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 414ms/step - accuracy: 0.9518 - loss: 0.1552 - val_accuracy: 0.9310 - val_loss: 0.2782\n"
     ]
    }
   ],
   "source": [
    "history  = model.fit(X_train,y_train, epochs=15,verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"skin_cancer_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization,InputLayer\n",
    "from tensorflow.keras.applications import VGG16,ResNet50 \n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232 images belonging to 2 classes.\n",
      "Found 56 images belonging to 2 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bugra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5s/step - accuracy: 0.6231 - loss: 2.5360 - val_accuracy: 0.7500 - val_loss: 0.7862\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5s/step - accuracy: 0.6525 - loss: 0.9223 - val_accuracy: 0.7143 - val_loss: 0.5754\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5s/step - accuracy: 0.8539 - loss: 0.4203 - val_accuracy: 0.8214 - val_loss: 0.3621\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5s/step - accuracy: 0.8441 - loss: 0.2971 - val_accuracy: 0.8214 - val_loss: 0.3429\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5s/step - accuracy: 0.9494 - loss: 0.1517 - val_accuracy: 0.8214 - val_loss: 0.3330\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 0.9692 - loss: 0.1160 - val_accuracy: 0.8214 - val_loss: 0.3719\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 0.9895 - loss: 0.0751 - val_accuracy: 0.8036 - val_loss: 0.3591\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5s/step - accuracy: 0.9913 - loss: 0.0645 - val_accuracy: 0.8571 - val_loss: 0.3388\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5s/step - accuracy: 0.9935 - loss: 0.0436 - val_accuracy: 0.8214 - val_loss: 0.3276\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5s/step - accuracy: 0.9881 - loss: 0.0514 - val_accuracy: 0.8571 - val_loss: 0.3412\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"skin-cancer/\"\n",
    "img_width, img_height = 224, 224 # Image dimensions\n",
    "\n",
    "id = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Data generator\n",
    "train_datagen_generator = id.flow_from_directory(directory=data_dir, target_size=(img_width, img_height), class_mode='binary', subset='training')\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # Data generator\n",
    "test_datagen_generator = id.flow_from_directory(directory=data_dir, target_size=(img_width, img_height), class_mode='binary', subset='validation')\n",
    "\n",
    "base_model = VGG16(weights='imagenet', input_shape=(img_width, img_height, 3),include_top=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_datagen_generator, epochs=10, validation_data=test_datagen_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">138,357,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │   \u001b[38;5;34m138,357,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,025,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,025\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,435,693</span> (539.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m141,435,693\u001b[0m (539.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,026,049</span> (3.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,026,049\u001b[0m (3.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,357,544</span> (527.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m138,357,544\u001b[0m (527.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,052,100</span> (7.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,052,100\u001b[0m (7.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.0303 - val_accuracy: 0.8393 - val_loss: 0.3274\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.0277 - val_accuracy: 0.8571 - val_loss: 0.3678\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.9898 - loss: 0.0281 - val_accuracy: 0.8393 - val_loss: 0.3396\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6s/step - accuracy: 0.9957 - loss: 0.0317 - val_accuracy: 0.8393 - val_loss: 0.3607\n"
     ]
    }
   ],
   "source": [
    "# early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(train_datagen_generator, epochs=10, validation_data=test_datagen_generator, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the class of the image with `ResNet50`. (warplane image)"
   ]
  },
  {
   "attachments": {
    "img.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img.jpg](attachment:img.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('n04552348', 'warplane', 0.96160185),\n",
       "  ('n03773504', 'missile', 0.014286742),\n",
       "  ('n04008634', 'projectile', 0.005633849)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions \n",
    "# Preprocess input: scale input to the range of the trained model \n",
    "# Decode predictions: convert the prediction to human-readable labels\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(\"img.jpg\", target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_batch = np.expand_dims(img_array, axis=0)\n",
    "img_preprocessed = preprocess_input(img_batch)\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "prediction = model.predict(img_preprocessed)\n",
    "\n",
    "label = decode_predictions(prediction, top=3)\n",
    "\n",
    "label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
